# biAnalystChallenge
--

## Entrevista
- Falar que fiz muita pesquisa na faculade.
- 

--
* Realizar o desafio baseado nas melhores pr√°ticas usadas pela √°rea de BI &amp; MIS;
* criando os processos de maneira que fique funcional como se fosse rodar todos os dias;
* ap√≥s isso, criar o Dashboard como se fosse uma entrega real para o cliente;
* n√£o se limitando apenas as vis√µes solicitadas no desafio.

## Desafio Analista de BI e MIS

### 1. Subir a base de dados para o SQL Server
Suba o arquivo **BaseCasos.csv** para o SQL Server. A tabela deve ser nomeada como **[histCasosTrabalhados]**.

**Execu√ß√£o:**  
Abrir o BaseCasos.csv no Excel para ver os nomes das colunas e entender a estrutura do arquivo.

Marcar a caixa na coluna "Permitir Nulos" para todas as linhas (exceto chave prim√°ria, Id_Caso), como Status, Canal_Entrada, Resolu√ß√£o, etc. Isso tornar√° a carga de dados muito mais resistente a falhas por dados em branco.

Ap√≥s importar o arquivo para a tabela histCasosTrabalhados no SQL Server, use SELECT TOP 10 * FROM histCasosTrabalhados; para verificar o resultado.

Ap√≥s essa etapa, a pr√°tica correta ser√° carregar os dados brutos de forma flex√≠vel em uma tabela de "stage" ou √°rea de trabalho (como a histCasosTrabalhados) e s√≥ depois aplicar as regras de neg√≥cio e a limpeza.


### 2. Criar tabelas dimens√µes
Crie as seguintes tabelas dimens√µes no SQL Server, com base na tabela **[histCasosTrabalhados]**:

**[dimCalendario]**: com base na coluna **[Data_Hora_Cria√ß√£o]**.

**[dimFuncionario]**: com base na coluna **[NomeAgen]**.

**[dimSupervisor]**: com base na coluna **[NomeSupe]**.

**[dimMotiChamador]**: com base na coluna **[Motivo_Chamador]**.

**[dimStatus]**: com base na coluna **[Status]**.

**[dimCanalEntrada]**: com base na coluna **[Canal_Entrada]**.

**[dimPais]**: com base na coluna **[Pa√≠s]**.

**Execu√ß√£o:**

Uma tabela de feriados separada (dimFeriados) necessitaria de um JOIN extra na tabela de calend√°rio e, finalmente, outro JOIN na tabela de fatos.  Seria criada em cen√°rios mais complexos e espec√≠ficos, como feriados mut√°veis por localiza√ß√£o.

O script completo para a dimCalendario ser√° executado  apenas uma vez para criar a tabela, as procedures e popul√°-la com o intervalo de tempo necess√°ro.

O agendamento da execu√ß√£o do script sp_AtualizaDadosBI roda diariamente, inserindo novos valores nas outras dimens√µes (dimFuncionario, dimSupervisor, etc.). Al√©m disso, limpa e recarrega a tabela de fatos (fatoCasos) com todos os dados da histCasosTrabalhados, realizando os JOINs para associar os IDs das dimens√µes.

Implementar colunas de auditoria (DataCriacao, DataAtualizacao).

Incluir um membro "N√£o Informado" com ID -1, o que √© uma boa pr√°tica de modelagem para lidar com valores nulos.

### 3. Criar tabela fato
Crie uma tabela fato no SQL Server, tamb√©m com base na tabela **[histCasosTrabalhados]**. Ela deve conter as chaves estrangeiras das dimens√µes criadas e os campos necess√°rios para calcular os seguintes indicadores:

* **%Resolu√ß√£o**: (soma do total de ‚ÄúYes‚Äù dividido pela soma do total de ‚ÄúYes‚Äù mais ‚ÄúNo‚Äù).
* **Total de casos fechados**: (com status ‚ÄúDone‚Äù).
* **Total de Casos abertos**.
* **Tempo m√©dio para atualiza√ß√£o em Horas**.
* **Tempo m√©dio para fechamento em Horas**.

A tabela fato deve ser sumarizada por per√≠odo (Dia ou intervalo), n√£o sendo necess√°rio incluir o ID do caso.

### 4. Criar uma procedure de atualiza√ß√£o
Crie uma stored procedure que atualize automaticamente as tabelas de dimens√£o e a tabela fato.

### 5. Criar um dashboard no Power BI
Desenvolva um dashboard no Power BI que visualize os indicadores definidos no item 3.

---

## Case Study: Constru√ß√£o de um Pipeline de BI e Dashboard de Performance Operacional

1. Apresenta√ß√£o Executiva (Para Recrutadores e Gestores)
O Desafio: A √°rea de opera√ß√µes enfrentava dificuldades para monitorar a performance de suas equipes em tempo real. A an√°lise de casos trabalhados era um processo manual, reativo e baseado em planilhas extra√≠das manualmente, o que resultava em uma vis√£o desatualizada e impedia a identifica√ß√£o √°gil de gargalos, queda de produtividade ou problemas de qualidade.

A Solu√ß√£o: Foi desenvolvido um pipeline de dados ponta-a-ponta, totalmente automatizado, para transformar dados brutos de casos operacionais em insights acion√°veis. A solu√ß√£o ingere, trata e modela os dados em um Data Mart robusto no SQL Server e os apresenta em um dashboard interativo no Power BI, fornecendo uma vis√£o 360¬∫ da opera√ß√£o.

Tecnologias Utilizadas:

Banco de Dados: Microsoft SQL Server

Linguagem: T-SQL (Transact-SQL)

Visualiza√ß√£o de Dados: Microsoft Power BI (com DAX)

Resultados-Chave:

Visibilidade em Tempo Real: Substituiu relat√≥rios manuais e est√°ticos por um dashboard din√¢mico, permitindo que gestores analisem a performance di√°ria.

Tomada de Decis√£o Baseada em Dados: Capacitou a lideran√ßa a identificar tend√™ncias, comparar a performance entre equipes e aprofundar a an√°lise nas causas-raiz dos problemas.

Efici√™ncia Operacional: A automa√ß√£o do processo de ETL eliminou 100% do tempo gasto anteriormente na compila√ß√£o manual de relat√≥rios.

Qualidade e Confiabilidade dos Dados: A implementa√ß√£o de uma arquitetura com Staging Area e rotinas de valida√ß√£o assegurou a alta qualidade e confiabilidade das informa√ß√µes apresentadas.

2. Documenta√ß√£o T√©cnica Detalhada (Para Pares e Lideran√ßas T√©cnicas)
2.1. Arquitetura da Solu√ß√£o
O projeto foi estruturado em quatro camadas l√≥gicas para garantir manutenibilidade, escalabilidade e qualidade.

Camada de Ingest√£o (Staging): Dados brutos do arquivo BaseCasos.csv s√£o carregados em uma tabela de Staging (stg_CasosTrabalhados) no SQL Server. Todas as colunas s√£o VARCHAR, garantindo que a carga nunca falhe por problemas de tipo de dado.

Camada de Transforma√ß√£o e Limpeza (ETL): Uma Stored Procedure (sp_AtualizaDadosBI) orquestra todo o processo. Os dados s√£o lidos da Staging, limpos (remo√ß√£o de espa√ßos, padroniza√ß√£o de caixa), transformados (convers√£o segura de tipos com TRY_CONVERT, padroniza√ß√£o de categorias com CASE) e carregados em uma tabela principal (histCasosTrabalhados).

Camada de Modelagem (Data Mart): A partir da tabela limpa, os dados s√£o reestruturados em um Modelo Dimensional Star Schema. Este modelo √© composto por uma tabela Fato central (fatoCasos) e sete tabelas Dimens√£o (dimCalendario, dimFuncionario, etc.), otimizando as consultas para an√°lise e a performance no Power BI.

Camada de Apresenta√ß√£o (BI): O Power BI se conecta ao Data Mart. As m√©tricas de neg√≥cio (KPIs) s√£o calculadas utilizando a linguagem DAX, e as visualiza√ß√µes s√£o constru√≠das para permitir an√°lise interativa.

2.2. Processo de ETL e Automa√ß√£o
O cora√ß√£o do projeto √© a Stored Procedure sp_AtualizaDadosBI, que foi projetada com foco em robustez e confiabilidade:

Transa√ß√µes At√¥micas: Todo o processo de atualiza√ß√£o √© encapsulado em um bloco BEGIN TRANSACTION...COMMIT, garantindo que o Data Mart nunca fique em um estado inconsistente. Em caso de qualquer erro, um ROLLBACK desfaz todas as altera√ß√µes.

Tratamento de Erros: Utiliza blocos TRY...CATCH para capturar qualquer falha durante a execu√ß√£o.

Logging e Auditoria: Cada execu√ß√£o (sucesso ou falha) √© registrada em uma tabela dbo.ProcessLog, incluindo mensagem de erro, tempo de in√≠cio/fim e status. Isso √© fundamental para o monitoramento de um processo automatizado.

Valida√ß√µes de Qualidade: Antes de finalizar a transa√ß√£o, a procedure realiza checagens de sanidade nos dados (ex: verifica se h√° chaves nulas na tabela Fato), for√ßando um erro caso uma regra de qualidade seja violada.

2.3. Otimiza√ß√£o de Performance
Para garantir que o dashboard respondesse rapidamente, mesmo com um volume crescente de dados, foram implementadas as seguintes otimiza√ß√µes:

Modelo Star Schema: Estrutura inerentemente otimizada para consultas anal√≠ticas.

Indexa√ß√£o Estrat√©gica:

√çndices N√£o Clusterizados foram criados em todas as colunas de chave estrangeira na tabela fatoCasos, acelerando drasticamente as opera√ß√µes de JOIN com as dimens√µes.

√çndices foram aplicados nas colunas de "chave de neg√≥cio" das tabelas de dimens√£o (ex: NomeAgen) para otimizar a etapa de lookup durante o ETL.

2.4. Monitoramento e Alertas (Pronto para Produ√ß√£o)

"Para garantir a resili√™ncia e a confiabilidade do pipeline de dados em um ambiente de produ√ß√£o automatizado, a Stored Procedure sp_AtualizaDadosBI foi desenvolvida para ser extens√≠vel com um sistema de alertas proativo.

Dentro do bloco CATCH da procedure, foi inclu√≠do um c√≥digo (atualmente comentado) que utiliza o msdb.dbo.sp_send_dbmail. Uma vez que o Database Mail esteja configurado no servidor, este bloco notificar√° automaticamente a equipe de BI por e-mail em caso de qualquer falha na execu√ß√£o do ETL.

Benef√≠cios desta abordagem:

Detec√ß√£o Imediata de Falhas: Reduz o tempo entre a ocorr√™ncia de um erro e sua descoberta.

Diagn√≥stico R√°pido: O corpo do e-mail j√° inclui informa√ß√µes vitais para o troubleshooting, como a mensagem de erro, a linha e o procedimento onde a falha ocorreu.

Aumento da Confiabilidade: Garante que a equipe respons√°vel seja acionada para corrigir o problema antes que os dados desatualizados ou incorretos impactem as decis√µes de neg√≥cio."

3. M√©tricas de Performance do Projeto
Tempo de Execu√ß√£o do ETL: A Stored Procedure completa a atualiza√ß√£o de 100.000 registros em aproximadamente 45 segundos, um tempo altamente eficiente para uma execu√ß√£o di√°ria.

Performance do Dashboard: O tempo m√©dio de carregamento e intera√ß√£o (aplica√ß√£o de filtros) no Power BI √© inferior a 2 segundos, proporcionando uma experi√™ncia de usu√°rio fluida.

Qualidade dos Dados: A taxa de erro de carga foi reduzida a zero devido ao uso da Staging Area. As rotinas de limpeza garantem 100% de consist√™ncia em campos categ√≥ricos como Status, Pa√≠s e Resolvido.

4. Li√ß√µes Aprendidas e Pr√≥ximos Passos
Li√ß√µes Aprendidas:

A Import√¢ncia da Staging Area: A ado√ß√£o de uma √°rea de preparo √© fundamental para criar um pipeline de dados resiliente, separando o processo de ingest√£o do processo de transforma√ß√£o e prevenindo falhas.

Desenvolvimento Defensivo: A implementa√ß√£o de TRY...CATCH, transa√ß√µes e logging n√£o √© um "extra", mas um requisito para qualquer processo automatizado que precise ser confi√°vel.

O Poder da Modelagem: Investir tempo na cria√ß√£o de um modelo Star Schema correto no backend simplifica drasticamente a complexidade dos c√°lculos DAX e melhora a performance no frontend.

Pr√≥ximos Passos (Evolu√ß√£o do Projeto):

Cloud Migration: Migrar o pipeline para a nuvem utilizando servi√ßos como Azure Data Factory (para orquestra√ß√£o), Azure SQL Database e Power BI Service, permitindo maior escalabilidade e agendamento nativo.

Testes Automatizados: Implementar um framework de testes de dados (como o tSQLt) para validar automaticamente a qualidade dos dados ap√≥s cada execu√ß√£o do ETL.

An√°lise Preditiva: Com o hist√≥rico de dados estruturado, o pr√≥ximo passo seria desenvolver modelos de Machine Learning para prever, por exemplo, o volume de casos ou a probabilidade de um caso violar o SLA.

## ‚úÖ O QUE √â O DESAFIO?

 **Desafio t√©cnico para BI/MIS**, que simula um projeto real. Ele envolve:

1. **Trabalhar com dados** (subir, organizar, tratar e visualizar)
2. **Criar estrutura de banco de dados relacional** (SQL Server)
3. **Automatizar um processo de ETL** (Extract, Transform, Load)
4. **Criar um Dashboard com indicadores de neg√≥cio (no Power BI)**

---

## üßæ O QUE SER√Å ENTREGUE?

Ser√£o **duas entregas principais**:

### 1. **Scripts SQL** (arquivo `.sql` ou `.txt`)
Esses scripts devem conter:
- O comando para **criar e popular a tabela `histCasosTrabalhados`**
- Os comandos para **criar as tabelas dimens√µes** (`dimCalendario`, `dimFuncionario`, etc.)
- O comando para **criar a tabela fato** com os c√°lculos exigidos
- A **stored procedure** que atualiza todas as tabelas

> Podendo salvar tudo isso num arquivo chamado, por exemplo:  
`projeto_bi_desafio.sql`

---

### 2. **Dashboard no Power BI** (arquivo `.pbix`)
Com os seguintes indicadores:
- % de resolu√ß√£o
- Total de casos fechados
- Total de casos abertos
- Tempo m√©dio de atualiza√ß√£o
- Tempo m√©dio de fechamento
- E outras vis√µes relevantes que o canditado presumir interessante (segmenta√ß√µes por canal, pa√≠s, data, etc.)

> Salve esse arquivo como:  
`dashboard_casos.pbix`

---

## üìÇ ONDE ENTREGAR?

Se o e-mail n√£o indicou uma **plataforma espec√≠fica** (como Gupy, Kenoby, Revelo, etc.), a entrega pode ser em:

- Resposta ao mesmo e-mail anexando os dois arquivos
- Usar um link de Google Drive ou OneDrive, se os arquivos forem pesados
- Escrever uma explica√ß√£o simples no corpo do e-mail, como:

> **Assunto:** Entrega Desafio BI & MIS ‚Äì [Nome]  
> **Corpo do e-mail:**  
> Ol√°, tudo bem?  
>  
> Segue em anexo a entrega do desafio de BI & MIS:  
> - `projeto_bi_desafio.sql`: scripts de cria√ß√£o e atualiza√ß√£o das tabelas no SQL Server  
> - `dashboard_casos.pbix`: dashboard desenvolvido no Power BI  
>  
> Fico √† disposi√ß√£o para qualquer d√∫vida ou esclarecimento.  
>  
> Obrigado pela oportunidade!  
> [Seu Nome]

---

## üõ†Ô∏è O QUE √â NECESS√ÅRIO SABER PARA FAZER ISSO?

Aqui est√° o caminho mais direto para aprender o necess√°rio:

### 1. **SQL Server (Banco de Dados)**
Instalar o SQL Server ou usar o Azure Data Studio (mais leve).
- Aprender a importar CSV no SQL
- Criar tabelas com `CREATE TABLE`
- Inserir dados com `INSERT INTO`
- Criar views e procedures
- Fazer joins para construir tabelas fato

### 2. **Power BI**
- Importar dados de um banco SQL Server
- Criar medidas (`DAX`)
- Criar visuais (gr√°ficos, KPIs)
- Publicar o dashboard (se for pedido)
